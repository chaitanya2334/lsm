import torch
from src.pytorch_models import utils
from src.pytorch_models.mlp import MLP
from src.pytorch_models.s2t_self_attention import SelfAttentionS2T
from torch import BoolTensor, FloatTensor, LongTensor, nn
from torch.nn import Embedding


class SpanExtractor(nn.Module):
    def __init__(
        self,
        input_dim: int,
        out_dim: int,
        dropout: float = 0,
        max_width: int = 8,
        max_pos: int = 50,
        max_step: int = 5,
        pos_step_emb: bool = True,
        emb_dim: int = 100,
        s2t: bool = False
    ) -> None:
        """
        Generates a span representation based on a specific combination of its 
        tokens embeddings. 

        This module computes [x_s;x_e;softhead(x);span_width_emb(x)]
        
        where, 
        x_s = the first token embedding of the span
        x_e = the last token embedding of the span
        softhead(x) = s2t attention output for all tokens of the span
        span_width_emb(x) = span width embedding.

        Args:
            input_dim (int): The token embedding dimension.
            max_width (int): The maximum span width.
            width_emb_dim (int): The span width embedding dimensions
        """
        super().__init__()
        self.input_dim = input_dim
        self.max_width = max_width
        self.emb_dim = emb_dim
        self.max_pos = max_pos
        self.max_step = max_step

        self.span_width_embedding = Embedding(
            num_embeddings=self.max_width, embedding_dim=self.emb_dim
        )
        self.pos_embedding = Embedding(
            num_embeddings=self.max_pos, embedding_dim=self.emb_dim
        )
        self.step_embedding = Embedding(
            num_embeddings=self.max_step, embedding_dim=self.emb_dim
        )
        if s2t:
            self.s2t_attention = SelfAttentionS2T(
                emb_dim=self.input_dim,
                value_out_dim=emb_dim,
                use_value_mlp=True,
            )

        self.pos_step_emb = pos_step_emb
        self.s2t = s2t

        self.span_mlp = MLP(
            input_features=self.get_output_dim(),
            output_features=out_dim,
            arch=[int(self.get_output_dim() / 2)],
            act='relu',
            dropout=0.1,
            layernorm=False,
            bias=True,
        )

    def get_input_dim(self):
        return self.input_dim

    def get_output_dim(self):
        # [x_s;x_e;softhead(x);span_width_emb(x);pos_emb(x);step_emb(x)]
        s2t = self.s2t_attention.get_output_dim() if self.s2t else 0
        pos_step = self.emb_dim * 2 if self.pos_step_emb else 0

        return self.input_dim * 2 + s2t + self.emb_dim + pos_step

    def forward(
        self,
        sequences: FloatTensor,
        spans: LongTensor,
        sequence_mask: BoolTensor,
        span_mask: BoolTensor = None
    ):
        """
        Extract span representations from a `sequences` tensor of size 
        (batch_size, max_seq_len, emb_size). The span representation is computed
        by concatenating endpoints (start and end token positions) representation 
        in the `sequences` tensor in addition to a soft head 
        representation generated by applying self attention on all the tokens 
        within the span.

        Args:
            sequences (FloatTensor): A 3d tensor (batch_size, max_seq_len, 
                emb_size) storing token embeddings, typically derived from bert 
                models.
            spans (LongTensor): A 3d tensor (batch_size, num_spans, 2) 
                containing start and end position of spans within each sequence
                in `sequences`. 
            sequence_mask (BoolTensor): A 2d tensor (batch_size, 
                max_seq_len) used to mask out padded sequences.
            span_mask (BoolTensor, optional): A 2d tensor (batch_size, 
                num_spans) used to mask out spans. Useful in masking out spans 
                that we do not wish to train on. This is None during inference, 
                where we want all candiates evaluated. Defaults to None.
        """

        batch_size, num_spans, _ = spans.shape

        start, end = [col.squeeze(-1) for col in spans.split(1, -1)]
        # -> start -> (batch_size, num_spans)
        # -> end -> (batch_size, num_spans)

        # TODO remove hardcoded device
        step = torch.arange(0, batch_size, device="cuda:0").unsqueeze(1)
        step = step.expand(-1, num_spans)
        # -> (batch_size, num_spans)

        # apply span mask
        if span_mask is not None:
            start = start * span_mask
            end = end * span_mask

        # Step 1: x_s
        start_emb = utils.batched_idx_select(sequences, start)
        # -> (batch_size, num_spans, emb_size)

        # Step 2: x_e
        end_emb = utils.batched_idx_select(sequences, end)
        # -> (batch_size, num_spans, emb_size)

        # Step 3: attention over spans
        span_embeddings, mask = utils.batched_span_select(sequences, spans)
        # -> span_embeddings -> (batch_size, num_spans, max_span_width, emb_dim)
        # -> mask -> (batch_size, num_spans, max_span_width)

        batch_size, num_spans, max_span_width, emb_dim = span_embeddings.shape
        span_embeddings = span_embeddings.view(-1, max_span_width, emb_dim)
        # -> (batch_size * num_spans, max_span_width, emb_dim)

        mask = mask.view(-1, max_span_width)
        # -> (batch_size * num_spans, max_span_width)

        softhead, att_weights = self.s2t_attention(span_embeddings, mask)
        # -> (batch_size * num_spans, s2t_out_dim)
        att_weights = att_weights.squeeze().view(batch_size, num_spans, -1)

        softhead = softhead.view(batch_size, num_spans, -1)
        # -> (batch_size, num_spans, s2t_out_dim)

        # Step 4: span_width(x). make sure the index does not exceed
        # embedding table size.
        span_width_emb = self.span_width_embedding(
            torch.clamp(end - start, max=self.max_width - 1)
        )
        # -> (batch_size, num_spans, width_emb_dim)

        if self.pos_step_emb:
            # Step 5: pos(x). span's starting position in its sentence.
            pos_emb = self.pos_embedding(
                torch.clamp(start, max=self.max_pos - 1)
            )
            # -> (batch_size, num_spans, pos_emb_dim)

            # Step 6: step_pos(x). span's position in steps.
            step_emb = self.step_embedding(
                torch.clamp(step, max=self.max_step - 1)
            )
            # -> (batch_size, num_spans, step_emb_dim)

            # [x_s;x_e;softhead(x);span_width(x);pos_emb(x);step_emb(x)]
            out = torch.cat(
                [
                    start_emb,
                    end_emb,
                    softhead,
                    span_width_emb,
                    pos_emb,
                    step_emb
                ],
                dim=-1
            )
        else:
            # [x_s;x_e;softhead(x);span_width(x)]
            out = torch.cat(
                [start_emb, end_emb, softhead, span_width_emb], dim=-1
            )

        # apply span_mask
        if span_mask is not None:
            out = out * span_mask.unsqueeze(-1)

        batch_size, num_spans, _ = out.shape
        out = self.span_mlp(out.view(-1, out.shape[-1]))
        # -> (batch_size * num_spans, out_emb_size)

        out = out.view(batch_size, num_spans, -1)
        # -> (batch_size, num_spans, out_emb_size)
        return out, att_weights
